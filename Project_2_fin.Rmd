---
title: "Project 2"
date: "30.11.2020"
author: "Anna Rybina"
output:
  html_document: #default
    toc: true
    #theme: united
    toc_depth: 5
    toc_float: true
    #number_section: true
#editor_options:
  chunk_output_type: console

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 0. Prerequisites: installation and import of libraries
The current work requires the following packages (R version: 3.6.3):
```{r message=FALSE, warning = FALSE}
require('MASS')
require('dplyr')
require('ggplot2')
require('cowplot')
require('corrplot')
require('car')
require('reshape2')
require('RColorBrewer')
```

### 1. Data investigation and preparation
The goal of this work is to analyze Boston data (from MASS package) to reveal how the median house value (`mdev`), in Boston Suburbs, might depend on the following predictors:

* `crim`, per capita crime rate by town
* `zn`, proportion of residential land zoned for lots over 25,000 sq.ft
* `indus`, proportion of non-retail business acres per town
* `chas`, Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
* `nox`, nitric oxides concentration (parts per 10 million)
* `rm`, average number of rooms per dwelling
* `age`, proportion of owner-occupied units built prior to 1940
* `dis`, weighted distances to five Boston employment centres
* `rad`, index of accessibility to radial highways
* `tax`, full-value property-tax rate per USD 10,000
* `ptratio`, pupil-teacher ratio by town
* `black`, 1000(B - 0.63)^2 where B is the proportion of blacks by town
* `lstat`, percentage of lower status of the population

Dependent variable is `medv` -  median value of owner-occupied homes (in 1000$)

#### 1.1. First look at the data 
```{r message=FALSE, warning = FALSE}
# data import
df <- Boston

str(df)
```

Inspect the data structure through simple visualization:
```{r message=FALSE, warning = FALSE}
plot(df)
```

At least 1 variable (`chas`) is discrete. Transform it into factor type:
```{r message=FALSE, warning = FALSE}
df <- df %>% mutate(chas = factor(chas, labels = c('RiverFar','RiverClose')))
str(df)
```


#### 1.2. Check missing values
Boston data consists of  ```r dim(df)[1]``` rows (observations) and ```r dim(df)[2]``` columns (variables) and does not contain missing values:
```{r message=FALSE, warning = FALSE}
colSums(is.na(df))
```

#### 1.3. Detect outliers

##### 1.3.1. Cleveland dot plot and boxplot for dependent variable `medv`

Constructing Cleveland dot plot:
```{r message=FALSE, warning = FALSE}
theme_set(theme_bw())
ggplot(df, aes(y = 1:nrow(df), x = medv)) + 
  geom_point() + 
  labs(y = 'Index', 
       x = 'Value of variable (medv)', 
       title = "Cleveland dot plot for the median value of owner-occupied homes (medv)") + 
  theme(text = element_text(size=12), 
        axis.text = element_text(size=12))
```


Constructing boxplot:
```{r message=FALSE, warning = FALSE}
theme_set(theme_bw())
ggplot(df, aes(y = medv, x = chas, fill = factor(chas))) + 
  geom_boxplot() + 
  labs(y = 'Median value of owner-occupied home (medv)', 
       x = 'Proximity to the Charles River', 
       title = "Cleveland dot plot for median value of owner-occupied homes (medv)" ) + 
  
  scale_fill_hue(name = "Proximity to the Charles River", 
                 labels=c("close", "far")) +
  scale_x_discrete(labels =c("close", "far")) + 
  theme(text = element_text(size=12), 
        axis.text = element_text(size=12))

```

No critical outliers are observed for dependent variable `medv`.

##### 1.3.2. Cleveland dot plots and boxplots for independent variables (except for `chas`)

Constructing Cleveland dot plots:
```{r message=FALSE, warning = FALSE}
gg_dot <- ggplot(df, aes(y = 1:nrow(df))) + 
  ylab('Index')
Pl1 <- gg_dot + aes(x = crim) + geom_point(color = 'deeppink3') 
Pl2 <- gg_dot + aes(x = zn) + geom_point(color = "darkviolet")
Pl3 <- gg_dot + aes(x = indus) + geom_point(color = "darkgreen") 
Pl4 <- gg_dot + aes(x = nox) + geom_point(color = "darkorange" )
Pl5 <- gg_dot + aes(x = rm) + geom_point(color = "darkblue" )
Pl6 <- gg_dot + aes(x = age) + geom_point(color =  "cyan2" )
Pl7 <- gg_dot + aes(x = dis) + geom_point(color =  "coral2" )
Pl8 <- gg_dot + aes(x = rad)  + geom_point(color = "chartreuse2")
Pl9 <- gg_dot + aes(x = tax) + geom_point(color = "burlywood4")
Pl10 <- gg_dot + aes(x = ptratio) + geom_point(color = "yellow2" )
Pl11 <- gg_dot + aes(x = black) + geom_point(color = "gray")
Pl12 <- gg_dot + aes(x = lstat) + geom_point(color = "red2")
title_theme <- ggdraw() + 
  draw_label("Cleveland dot plots for variables from Boston dataset", x = 0, hjust = 0)
plot_row <- plot_grid(Pl1, Pl2, Pl3, Pl4, Pl5, Pl6,
          Pl7, Pl8, Pl9,Pl10,Pl11,Pl12, ncol = 3, nrow = 4)
plot_grid(title_theme, plot_row, ncol = 1, rel_heights = c(0.1, 1))
```


In general, almost all variables seem to be distributed without critical outliers. There are variables `tax` and `rad`  with some probable outliers but these data points might cover important observations. 

Boxplots illustrate that variables `crim`, `zn`, and `black` could have data points that differ significantly from other observations: 
```{r message=FALSE, warning = FALSE}
ggplot(data = melt(df[,-c(4, 14)] ), aes(x=variable, y=value)) + 
  geom_boxplot(fill = c('deeppink3', "darkviolet","darkgreen","darkorange","darkblue", "cyan2","coral2","chartreuse2","burlywood4","yellow2","gray","red2")) +
  facet_wrap( ~ variable, scales="free") + 
  xlab("Independent variable") +
  ylab("Value") + 
  labs(title = "Boxplots for independent variables")+
  theme_bw()
```

Nevertheless, data points resembling outliers could be informative and useful for our study and might reflect natural diversity of the studied object. We would not remove these candidate outliers. 


##### 1.3.3. Check discrete variable `chas`:
```{r message=FALSE, warning = FALSE}
table(df$chas)
```

We should keep in mind that there are more observations when the river is close to the house when it is not. 

#### 1.4. Data transformation - standardization
Applying scale function to dataframe, we standardize numeric variables measured in different units, which allow us further to compare variables and contribution of each of them on the same scale:
```{r message=FALSE, warning = FALSE}
df_scale <- as.data.frame(sapply(df[,-4], scale))
df_scale$chas <- df$chas
head(df_scale)
```

#### 1.5. Check the character of relationships between dependent variable and predictors

```{r message=FALSE, warning = FALSE}
gg_cor <- ggplot(df_scale, 
                 aes(y = medv)) 
Pl1 <- gg_cor + aes(x = crim) + geom_point(color = 'deeppink3') + geom_smooth( se = FALSE) 
Pl2 <- gg_cor + aes(x = zn) + geom_point(color = "darkviolet") + geom_smooth( se = FALSE) 
Pl3 <- gg_cor + aes(x = indus) + geom_point(color = "darkgreen") + geom_smooth( se = FALSE) 
Pl4 <- gg_cor + aes(x = nox) + geom_point(color = "darkorange" ) + geom_smooth( se = FALSE) 
Pl5 <- gg_cor + aes(x = rm) + geom_point(color = "darkblue" ) + geom_smooth( se = FALSE) 
Pl6 <- gg_cor + aes(x = age) + geom_point(color =  "cyan2" ) + geom_smooth( se = FALSE) 
Pl7 <- gg_cor + aes(x = dis) + geom_point(color =  "coral2" ) + geom_smooth( se = FALSE) 
Pl8 <- gg_cor + aes(x = rad)  + geom_point(color = "chartreuse2") + geom_smooth( se = FALSE) 
Pl9 <- gg_cor + aes(x = tax) + geom_point(color = "burlywood4") + geom_smooth( se = FALSE) 
Pl10 <- gg_cor + aes(x = ptratio) + geom_point(color = "yellow2" ) + geom_smooth( se = FALSE) 
Pl11 <- gg_cor + aes(x = black) + geom_point(color = "gray") + geom_smooth( se = FALSE) 
Pl12 <- gg_cor + aes(x = lstat) + geom_point(color = "red2") + geom_smooth( se = FALSE) 
title_theme <- ggdraw() + 
  draw_label("Scatter plot of dependent variables vs Median Value (medv)", x = 0, hjust = 0)
plot_row <- plot_grid(Pl1, Pl2, Pl3, Pl4, Pl5, Pl6,
          Pl7, Pl8, Pl9,Pl10,Pl11,Pl12, ncol = 3, nrow = 4)
plot_grid(title_theme, plot_row, ncol = 1, rel_heights = c(0.1, 1))
```

In general, there could be a linear relationship between dependent variable and predictors although the visualization is sensitive to gap and edge regions (probable outliers).   

#### 1.6. Check correlation between variables

```{r message=FALSE, warning = FALSE}
col_pal <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(cor(df_scale[, -14]), 
         method = "number", 
         col = col_pal(200),
         type = "lower", 
         diag = FALSE,
         title = "Correlation Matrix",
         tl.srt=45, mar=c(0,0,1,0), 
         addCoef.col = "black",
         tl.col="black", 
         number.cex = 0.7, 
         tl.cex = 1)
```


According to the correlation matrix, median value of owner-occupied homes  (`medv`) increases as average number of rooms per dwelling increases and percent of lower status population decreases.
Nitrogen oxides concentration (`nox`) is higher where radial highways are more accessible (`rad`) and where it is close to employment centres (`dis`) and in industrial areas (`indus`). We might expect drop in nitrogen oxides concentration in neighborhood with old buildings (`age`), probably far from the centre of town. `rad` and `tax` have a strong positive correlation of 0.91 suggesting that as accessibility of radial highways increases, the full value property-tax rate per $10,000 also increases. Strong association of `crim` with `rad` and `tax` implies that as accessibility to radial highways increases, per capita crime rate increases too.


### 2. Building full linear model w/o interactions

Let's first construct full model without interactions between variables:
```{r message=FALSE, warning = FALSE}
mod1 <- lm(medv ~ . , data = df_scale )
summary(mod1)

```


Primary full model (multiple linear regression without interactions) might be described by the following equation: 

1) if tract bounds Charles River:

**medv = `r round(coefficients(mod1)[['(Intercept)']],3) + round(coefficients(mod1)[[length(coefficients(mod1))]],3)` + (`r round(coefficients(mod1)[['crim']],3)`) \* crim + `r round(coefficients(mod1)[['zn']],3)` \* zn + `r round(coefficients(mod1)[['indus']],3)` \* indus + (`r round(coefficients(mod1)[['nox']],3)`) \* nox + `r round(coefficients(mod1)[['rm']],3)` \* rm + `r round(coefficients(mod1)[['age']],3)` \* age + (`r round(coefficients(mod1)[['dis']],3)`) \* dis + `r round(coefficients(mod1)[['rad']],3)` \* rad + (`r round(coefficients(mod1)[['tax']],3)`) \* tax + (`r round(coefficients(mod1)[['ptratio']],3)`) \* ptratio + `r round(coefficients(mod1)[['black']],3)`\*black + (`r round(coefficients(mod1)[['lstat']],3)`)\*lstat **

2) otherwise:

**medv = `r round(coefficients(mod1)[['(Intercept)']],3)` + (`r round(coefficients(mod1)[['crim']],3)`) \* crim + `r round(coefficients(mod1)[['zn']],3)` \* zn + `r round(coefficients(mod1)[['indus']],3)` \* indus + (`r round(coefficients(mod1)[['nox']],3)`) \* nox + `r round(coefficients(mod1)[['rm']],3)` \* rm + `r round(coefficients(mod1)[['age']],3)` \* age + (`r round(coefficients(mod1)[['dis']],3)`) \* dis + `r round(coefficients(mod1)[['rad']],3)` \* rad + (`r round(coefficients(mod1)[['tax']],3)`) \* tax + (`r round(coefficients(mod1)[['ptratio']],3)`) \* ptratio + `r round(coefficients(mod1)[['black']],3)` \* black + (`r round(coefficients(mod1)[['lstat']],3)`) \* lstat **

However, at 5 % significance level there is no association between `medv` and `age` (proportion of owner-occupied units built prior to 1940) as well as `indus` (proportion of non-retail business acres per town) (F-value: `r round(unname(summary(mod1)$fstatistic['value']),2)`, df: `r round(unname(summary(mod1)$fstatistic['numdf']),0)` and `r round(unname(summary(mod1)$fstatistic['dendf']),0)`). The model explains about `r round((summary(mod1))$adj.r.squared,2)*100` % of total variance of the data.

We may suggest that the median value of owner-occupied homes increases with increasing proportion of residential land zoned for lots over 25,000 sq.ft. (`zn`), average number of rooms per dwelling (`rm`), index of accessibility to radial highways (`rad`), proportion of blacks (`black`) and proximity to the river (`chas`). The following conditions might also facilitate to rising home price: low values of the crime rate (`crim`), nitrogen oxides concentration (`nox`), full-value property-tax rate (`tax`), and pupil-teacher ratio (`ptratio`) along with small mean distance to employment centres (`dis`) and small percentage of lower status population.  In other words, there are several factors which can maximize the home price: a safe and successful neighborhood, healthy environment combined with better accessibility to radial highways and employment centres, proximity to the river and high number of rooms per dwelling. 

We need to verify the model to be sure that it meets required assumptions. 

### 3. Linear regression diagnostics - model validation
Now we would validate assumptions made by linear mode. First, we check for absence in multicollinearity of predictors,  calculating the variance inflation factor (VIF). Second, analyze residuals and search for some suspecious patterns (if any) to validate linear relation between dependent variable and parameters, homoscedasticity of errors, independence of observations and  noraml distribution of residuals.  

#### 3.1. Multicollinearity check
Calculate  variance inflation factor (VIF). If VIF > 2 then remove respective presdictor and update the model:

**Step 1**:
```{r message=FALSE, warning = FALSE}
vif(mod1) # we should remove tax variable as its VIF is maximal and > 2 
```

**Step 2**:
```{r message=FALSE, warning = FALSE}
mod2 <- update(mod1, .~. - tax) # remove tax from the model
vif(mod2) # nox variable shows the highest VIF > 2
```


**Step 3**:
```{r message=FALSE, warning = FALSE}
mod3 <- update(mod2, .~. - nox) # remove nox from the model
vif(mod3) # dis variable shows the highest VIF > 2
```

**Step 4**:
```{r message=FALSE, warning = FALSE}
mod4 <- update(mod3, .~. - dis) # remove dis from the model
vif(mod4) # lstat variable shows the highest VIF > 2
```

**Step 5**:
```{r message=FALSE, warning = FALSE}
mod5 <- update(mod4, .~. - lstat) # remove lstat from the model
vif(mod5) # rad variable shows the highest VIF > 2
```

**Step 6**:
```{r message=FALSE, warning = FALSE}
mod6 <- update(mod5, .~. - rad) # remove rad from the model
vif(mod6) # indus variable shows the highest VIF > 2
```


**Step 7**:
```{r message=FALSE, warning = FALSE}
mod7 <- update(mod6, .~. - indus) # remove rad from the model
vif(mod7) # for each variable VIF < 2
```

```{r message=FALSE, warning = FALSE}
summary(mod7)
```

After multicollinearity analysis, the model is: 

1) if tract bounds Charles River:

**medv = `r round(coefficients(mod7)[['(Intercept)']],3) + round(coefficients(mod7)[[length(coefficients(mod7))]],3)` + (`r round(coefficients(mod7)[['crim']],3)`) \* crim + (`r round(coefficients(mod7)[['zn']],3)`) \* zn + `r round(coefficients(mod7)[['rm']],3)` \* rm + (`r round(coefficients(mod7)[['age']],3)`) \* age + (`r round(coefficients(mod7)[['ptratio']],3)`) \* ptratio + `r round(coefficients(mod7)[['black']],3)` \* black ** 

2) otherwise:

**medv =`r round(coefficients(mod7)[['(Intercept)']],3)` + (`r round(coefficients(mod7)[['crim']],3)`) \* crim + (`r round(coefficients(mod7)[['zn']],3)`) \* zn + `r round(coefficients(mod7)[['rm']],3)` \* rm + (`r round(coefficients(mod7)[['age']],3)`) \* age + (`r round(coefficients(mod7)[['ptratio']],3)`) \* ptratio + `r round(coefficients(mod7)[['black']],3)` \* black**

However, according to the results of t-test (df = 498), at 5% significance level median value of owner-occupied homes (`medv`) does not depend on variable the proportion of residential land zoned for lots (`zn`). We should further analyze the contribution of each predictor to the model, keep statistically significant ones. 


#### 3.2. Analysis of residuals

##### 3.2.1. Get data on residuals along with variables removed from the model:
```{r message=FALSE, warning = FALSE}
not_included1 <- colnames(df_scale)[!(colnames(df_scale) %in% names(coefficients(mod1))) & (colnames(df_scale)!='chas') & (colnames(df_scale)!='medv')]

mod1_diag <- fortify(mod1, df_scale %>% select(not_included1))
head(mod1_diag)
```

##### 3.2.2. Cook’s Distance plot:

No influential observations are found:
```{r message=FALSE, warning = FALSE}
ggplot(mod1_diag, 
       aes(x = 1:nrow(mod1_diag), 
           y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red") +
  xlab(label = "Index (observation number)") + 
  ylab(label = "Cook’s Distance") +
  labs(title = "Cook’s Distance plot")
```


##### 3.2.3. Standardized residuals vs Fitted values plot
```{r message=FALSE, warning = FALSE}
gg_resid <- ggplot(data = mod1_diag, 
                   aes(x = .fitted, 
                       y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red")
gg_resid + 
  xlab(label = "Fitted values") + 
  ylab(label = "Standardized residuals") +
  labs(title = 'Standardized residuals vs Fitted values plot')
```

Part of points are located out of the 2 standard deviations interval sugesting outliers that were not removed from the data. Still, the majority of observations is within 2 standard deviations interval.  
The mean residual value for every fitted value region is close to 0 which could put towards linearity of relationship between dependent variable and predictors. The spread of residuals is approximately the same across the x-axis but at the edge there is an assymmetry. We may suggest somewhat  homoskedasticity.     

##### 3.2.3. Quantile-comparison plot for standardized residuals
```{r message=FALSE, warning = FALSE}
qqPlot(mod1_diag$.stdresid,
       xlab = 'Theoretical quantiles of a normal distribution',
       ylab = 'Standardized residuals', 
       main = 'Quantile-comparison plot for standardised residuals')
```


The QQ-plot appears almost as a straight line although the end of the plot start to deviate. We may suggest the normal distribution of the residuals. 


### 4. Model optimiztion - backward selection
As we aimed to identify patterns/dependencies between `medv` and different predictors and interprete the model, we should try to optimize the model: keep only the most contributive predictiors. Moreover, overfitted model with a lot of parameters could start to predict the noise, might be work with different data and it is sometimes too hard to interprete.
We can check the significance of predictors using the partial F-test for comparison model with all variables  and a model without one of variable. We will follow  backward selection approach: start with all predictors in the model, iteratively remove the least contributive predictors, and stops whith a model where all predictors are statistically significant. 

```{r message=FALSE, warning = FALSE}
drop1(mod7, test = "F") # the last contributive predictor is zn
```


```{r message=FALSE, warning = FALSE}
mod8 <- update(mod7, .~. - zn) # remove zn variable from the last model
drop1(mod8, test = "F")
```
Now all predictors are statistically significant. 

```{r message=FALSE, warning = FALSE}
summary(mod8)
```

After the model diagnistics and backward selection, we obtained the following final model: 

1) if tract bounds Charles River:  

**medv = `r round(coefficients(mod8)[['(Intercept)']],3) + round(coefficients(mod8)[[length(coefficients(mod8))]],3)` +  (`r round(coefficients(mod8)[[2]],3)`) \* crim + `r round(coefficients(mod8)[[3]],3)` \* rm  + (`r round(coefficients(mod8)[[4]],3)`) \* age + (`r round(coefficients(mod8)[[5]],3)`) \* ptratio + `r round(coefficients(mod8)[[6]],3)` \* black**

2) otherwise: 

**medv = `r round(coefficients(mod8)[['(Intercept)']],3)` + (`r round(coefficients(mod8)[[2]],3)`) \* crim + `r round(coefficients(mod8)[['rm']],3)` \* rm  + (`r round(coefficients(mod8)[[4]],3)`) \* age + (`r round(coefficients(mod8)[[5]],3)`) \* ptratio + `r round(coefficients(mod8)[[6]],3)` \* black**


### 5. Diagnostics of the model after its optimiztion

#### 5.1. Get data on residuals along with variables removed from the model:
```{r message=FALSE, warning = FALSE}
not_included <- colnames(df_scale)[!(colnames(df_scale) %in% names(coefficients(mod8))) & (colnames(df_scale)!='chas') & (colnames(df_scale)!='medv')]

mod8_diag <- fortify(mod8, df_scale %>% select(not_included))
head(mod8_diag)
```

#### 5.2. Cook’s Distance plot:

```{r message=FALSE, warning = FALSE}
ggplot(mod8_diag, 
       aes(x = 1:nrow(mod8_diag), 
           y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red") +
  xlab(label = "Index (observation number)") + 
  ylab(label = "Cook’s Distance") +
  labs(title = "Cook’s Distance plot")
```
No influential observations are found.


#### 5.3. Standardized residuals vs Fitted values plot
```{r message=FALSE, warning = FALSE}
gg_resid <- ggplot(data = mod8_diag, 
                   aes(x = .fitted, 
                       y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red")
gg_resid + 
  xlab(label = "Fitted values") + 
  ylab(label = "Standardized residuals") +
  labs(title = 'Standardized residuals vs Fitted values plot')
```

Although there are some points that stand up to 7 standard devitions away, the majority of observations is within 2 standard deviations interval.  
The mean residual value for every fitted value region is close to 0 which could put towards linearity of relationship between dependent variable and predictors. The spread of residuals is approximately the same across the x-axis  but at the edge there is an assymmetry. We may suggest homoskedasticity however not perfect. 
Probably, we should have removed some outliers to obtain better model. 

#### 5.4. Quantile-comparison plot for standardised residuals
```{r message=FALSE, warning = FALSE}
qqPlot(mod8_diag$.stdresid,
       xlab = 'Theoretical quantiles of a normal distribution',
       ylab = 'Standardized residuals', 
       main = 'Quantile-comparison plot for standardised residuals')
```

 
We could notice a slight flipped S shape on the QQ plot. Standardized residuals might be over-dispersed relative to a normal distribution, probably due to some outliers in the data. 


#### 5.5. Standardized residuals vs predictirs that were not included into the model

```{r message=FALSE, warning = FALSE}
res_1 <- gg_resid + aes(x = zn) + ylab(label = "Residuals")
res_2 <- gg_resid + aes(x = indus) + ylab(label = "Residuals")
res_3 <- gg_resid + aes(x = nox) + ylab(label = "Residuals")
res_4 <- gg_resid + aes(x = dis) + ylab(label = "Residuals")
res_5 <- gg_resid + aes(x = rad) + ylab(label = "Residuals")
res_6 <- gg_resid + aes(x = tax) + ylab(label = "Residuals")
res_7 <- gg_resid + aes(x = lstat) + ylab(label = "Residuals")

plot_row <- plot_grid(res_1, res_2, res_3, 
                      res_4, res_5, res_6, res_7, nrow = 4)

title_theme <- ggdraw() + 
  draw_label("Standardized residuals vs predictirs removed from the model", x = 0, hjust = 0)

plot_grid(title_theme, plot_row, ncol = 1, rel_heights = c(0.1, 1))

```

No patterns are observed in constructed plots suggesting no need in including  independent variables `zn`, `indus`, `nox`, `dis`, `rad`, `tax`, and `lstat` into the model. 


To sum up, after the diagnostics and backward selection, we obtained the following model based on scaled data: 

1) if tract bounds Charles River:

medv = `r round(coefficients(mod8)[['(Intercept)']],3) + round(coefficients(mod8)[[length(coefficients(mod8))]],3)` +  (`r round(coefficients(mod8)[[2]],3)`) \* crim + `r round(coefficients(mod8)[[3]],3)` \* rm  + (`r round(coefficients(mod8)[[4]],3)`) \* age + (`r round(coefficients(mod8)[[5]],3)`) \ *ptratio + `r round(coefficients(mod8)[[6]],3)` \* black

2) otherwise: 

medv = `r round(coefficients(mod8)[['(Intercept)']],3)` +  (`r round(coefficients(mod8)[[2]],3)`) \* crim + `r round(coefficients(mod8)[[3]],3)` \* rm  + (`r round(coefficients(mod8)[[4]],3)`) \* age + (`r round(coefficients(mod8)[[5]],3)`) \* ptratio + `r round(coefficients(mod8)[[6]],3)` \* black


Accoriding to the final model, at 1 % significance level there is an association between the median value of owner-occupied homes (`medv`) and per capita crime rate by town (`crim`), average number of rooms per dwelling (`rm`), proportion of owner-occupied units built prior to 1940 (`age`), pupil-teacher ratio by town (`ptratio`),the proportion of blacks (`black`), and the proximity to the River (`chas`) (F-value: `r round(unname(summary(mod8)$fstatistic['value']),2)`, df: `r round(unname(summary(mod8)$fstatistic['numdf']),0)` and `r round(unname(summary(mod8)$fstatistic['dendf']),0)`). The model explains about `r round((summary(mod8))$adj.r.squared,2)*100` % of total variance of the data. 
The most contributive predictor is `rm` (average number of rooms per dwelling): when average number of rooms per dwelling changes by 1 standard deviation, the median value of owner-occupied homes changes by `r round(coefficients(mod8)[[3]],3)`. Сlose proximity to the river also incerases the home price. 



### 6. Build reduced linear model w/o interactions based on initial data. Model diagnostics. 

After the diagnostics and backward selection (steps 2 - 5), we obtained the following model based on scaled data: 

1) if tract bounds Charles River: 

medv = `r round(coefficients(mod8)[['(Intercept)']],3) + round(coefficients(mod8)[[length(coefficients(mod8))]],3)` +  (`r round(coefficients(mod8)[[2]],3)`) \* crim + `r round(coefficients(mod8)[[3]],3)` \* rm  + (`r round(coefficients(mod8)[[4]],3)`) \* age + (`r round(coefficients(mod8)[[5]],3)`) \ *ptratio + `r round(coefficients(mod8)[[6]],3)` \* black 

2) otherwise: 

medv = `r round(coefficients(mod8)[['(Intercept)']],3)` +  (`r round(coefficients(mod8)[[2]],3)`) \* crim + `r round(coefficients(mod8)[[3]],3)` \* rm  + (`r round(coefficients(mod8)[[4]],3)`) \* age + (`r round(coefficients(mod8)[[5]],3)`) \* ptratio + `r round(coefficients(mod8)[[6]],3)` \* black

We wil use these most contributive predictors (`char`, `crim`, `rm`, `age`, `ptratio`, and `black`) from optimized model derived from scaled data to construct model on initial data and make predictions.

#### 6.1. Build the linear model w/o interactions

Build the model bassed on initial non-standardized data with most contributive predictors obtained from previous steps:
```{r message=FALSE, warning = FALSE}
mod9 <- lm(medv ~ crim + rm + age + ptratio + black + chas, data = Boston %>% mutate(chas = factor(chas, labels = c('RiverFar','RiverClose'))))

summary(mod9)
```


All predictors are statistically significant according to partial F-test:
```{r message=FALSE, warning = FALSE}
drop1(mod9, test = "F")
```

The most contributive predictor is `rm` (average number of rooms per dwelling): for every increase in the average number of rooms per dwelling, the median value of owner-occupied homes goes up by `r round(coefficients(mod9)[['rm']],3) * 1000` $.

#### 6.2. Diagnostics. Model validation

##### 6.2.1. Multicollinearity check

Calculate variance inflation factor (VIF). If VIF > 2 then remove respective presdictor and update the model:
```{r message=FALSE, warning = FALSE}
vif(mod9) 
```

For each variable VIF is < 2, multicollinearity check is passed.

##### 6.2.2. Analysis of residuals

Get data on residuals along with variables removed from the model:
```{r message=FALSE, warning = FALSE}
not_included3 <- colnames(df)[!(colnames(df) %in% names(coefficients(mod9))) & (colnames(df)!='chas') & (colnames(df)!='medv')]

mod9_diag <- fortify(mod9, df %>% select(not_included3))
head(mod9_diag)
```


###### 6.2.2.1. Cook’s Distance plot:

No influential observations are found:
```{r message=FALSE, warning = FALSE}
ggplot(mod9_diag, 
       aes(x = 1:nrow(mod9_diag), 
           y = .cooksd)) + 
  geom_bar(stat = "identity") + 
  geom_hline(yintercept = 2, color = "red") +
  xlab(label = "Index (observation number)") + 
  ylab(label = "Cook’s Distance") +
  labs(title = "Cook’s Distance plot")
```


###### 6.2.2.2. Standardized residuals vs Fitted values plot

We obtained a plot similar to described above (see 5.3)
```{r message=FALSE, warning = FALSE}
gg_resid <- ggplot(data = mod9_diag, 
                   aes(x = .fitted, 
                       y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0) +
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red")
gg_resid + 
  xlab(label = "Fitted values") + 
  ylab(label = "Standardized residuals") +
  labs(title = 'Standardized residuals vs Fitted values plot')
```


###### 6.2.2.3. Quantile-comparison plot for standardized residuals

We obtained a plot similar to described above (see 5.4)
```{r message=FALSE, warning = FALSE}
qqPlot(mod9_diag$.stdresid,
       xlab = 'Theoretical quantiles of a normal distribution',
       ylab = 'Standardized residuals', 
       main = 'Quantile-comparison plot for standardised residuals')
```



###### 6.2.2.4. Standardized residuals vs predictors that were not included into the model

We obtained a plot similar to described above (see 5.5)

```{r message=FALSE, warning = FALSE}
res_1 <- gg_resid + aes(x = zn) + ylab(label = "Residuals")
res_2 <- gg_resid + aes(x = indus) + ylab(label = "Residuals")
res_3 <- gg_resid + aes(x = nox) + ylab(label = "Residuals")
res_4 <- gg_resid + aes(x = dis) + ylab(label = "Residuals")
res_5 <- gg_resid + aes(x = rad) + ylab(label = "Residuals")
res_6 <- gg_resid + aes(x = tax) + ylab(label = "Residuals")
res_7 <- gg_resid + aes(x = lstat) + ylab(label = "Residuals")

plot_row <- plot_grid(res_1, res_2, res_3, 
                      res_4, res_5, res_6, res_7, nrow = 4)

title_theme <- ggdraw() + 
  draw_label("Standardized residuals vs predictirs removed from the model", x = 0, hjust = 0)

plot_grid(title_theme, plot_row, ncol = 1, rel_heights = c(0.1, 1))
```



### 7. Prediction

For prediction, we will use the model that we obtained using initial data (model `mod9`). First we should construct test dataframe where the most contibutive predictor `rm` changes from its minimal to maximal value while other predictors are fixed to mean value. We consider that there might be different range of values for variable `rm` depending on the factor `chas`:

```{r message=FALSE, warning = FALSE}
new_data <- Boston %>% mutate(chas = factor(chas, labels = c('RiverFar','RiverClose'))) %>% group_by(chas)%>%
do(data.frame(rm = seq(min(.$rm), max(.$rm), length.out = 100),
              crim = mean(.$crim),
              age = mean(.$age),
              ptratio = mean(.$ptratio),
              black = mean(.$black)))
```



Calculate and add predictions:
```{r message=FALSE, warning = FALSE}
Predictions <- predict(mod9, 
                       newdata = new_data,
                       interval = 'confidence')



new_data <- data.frame(new_data, Predictions)
```


Create regression line with the prediction intervals:
```{r message=FALSE, warning = FALSE}
Pl_predict <- ggplot(new_data, 
                     aes(x = rm, y = fit)) +
  geom_ribbon(alpha = 0.2, 
              aes(ymin = lwr, ymax = upr, group = chas)) +
  geom_line(aes(colour = chas)) +
  geom_point(data = Boston %>% mutate(chas = factor(chas, labels = c('RiverFar','RiverClose'))), 
             aes(x = rm, y = medv, color = chas)) +
  ggtitle("Prediciton based on the multiple linear model with initial data points") +
  scale_colour_hue(name = "Proximity to the Charles River",
                   labels=c("close", "far")) +
  ylab(label = 'Fitted median value of owner-occupied homes in 1000$ \n (medv)') +
  xlab(label = 'Average number of rooms per dwelling \n (rm)')
Pl_predict
```


We may see that on average the median value of owner-occupied homes with equal number of rooms per dwelling should be higher for apartments which are close to the river than far ones. 



### 8. Final model description 




We obtained the following final model: 

1) if tract bounds Charles River:

medv = `r round(coefficients(mod9)[['(Intercept)']],3) + round(coefficients(mod9)[[length(coefficients(mod9))]],3)` +  (`r round(coefficients(mod9)[[2]],3)`) \* crim + `r round(coefficients(mod9)[[3]],3)` \* rm  + (`r round(coefficients(mod9)[[4]],3)`) \* age + (`r round(coefficients(mod9)[[5]],3)`) \ *ptratio + `r round(coefficients(mod9)[[6]],3)` \* black 

2) otherwise: 

medv = `r round(coefficients(mod9)[['(Intercept)']],3)` +  (`r round(coefficients(mod9)[[2]],3)`) \* crim + `r round(coefficients(mod9)[[3]],3)` \* rm  + (`r round(coefficients(mod9)[[4]],3)`) \* age + (`r round(coefficients(mod9)[[5]],3)`) \* ptratio + `r round(coefficients(mod9)[[6]],3)` \* black


Accoriding to the final reduced model, at 1 % significance level there is an association between the median value of owner-occupied homes (`medv`) and per capita crime rate by town (`crim`), average number of rooms per dwelling (`rm`), proportion of owner-occupied units built prior to 1940 (`age`), pupil-teacher ratio by town (`ptratio`),the proportion of blacks (`black`), and the proximity to the River (`chas`) (F-value: `r round(unname(summary(mod9)$fstatistic['value']),2)`, df: `r round(unname(summary(mod9)$fstatistic['numdf']),0)` and `r round(unname(summary(mod9)$fstatistic['dendf']),0)`). The model explains about `r round((summary(mod9))$adj.r.squared,2)*100` % of total variance of the data. 
The most contributive predictor is `rm` (average number of rooms per dwelling): for every increase in the average number of rooms per dwelling, the median value of owner-occupied homes goes up by `r round(coefficients(mod9)[[3]],3) * 1000` $. The close proximity to the river also incerases the home price.   
The median value should increase with increasing average number of rooms per dwelling (`rm`) and the proporton of blacks in town (`black`). The last observation probably suggests that the high proporton of blacks reflects the higher town population and the level of urbanization which results in higher home price. 
The median home value might be higher with the lower crime rate (`crim`) and lower pupil-teacher ratio (`ptratio`). Educational institutions with low pupil-teacher ratio are considered to be effective at teaching providing better education than those of high ratio. Overall, both low values of `crim` and `ptratio` point towards prosperous and safe neighborhood, on average leading to high home price.
Moreover, median value should rise when there are more new buildings in the neighborhood/less the old ones (predictor `age`). 

Overall, based on a final model, the following factors could maximize the median value of owner-occupied homes: high number of rooms per dwelling (`rm`), location at probably urbanized populated area (which might be reflected partially by high proportion of blacks `black`) with the high proportion of new buildings (low value of `age`), healty neighborhood (including close proximity to the river - parameter `chas`), and safe neighborhood (low per capita crime rate `crime` and low pupil-teacher ratio `ptratio`). 

Data contains some values that could be considered as ouliers although they might reflect heterogenity of the studied object and be informative and important for analysis. To come up eith better model without removing any data, we should probably consider different, non-linear approach.  


